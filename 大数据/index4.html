<html>
    <head>
    <title>Hadoop2.x集群编译安装</title>
    <meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type' />

<link href='/styles/github.css' rel='stylesheet' type='text/css' />
<script src='/javascripts/highlight.pack.js' type='text/javascript'></script>

<link href='/images/kbasha.png' rel='shortcut icon'>
<link href='/stylesheets/style.css' rel='stylesheet' type='text/css' />
<link href='/stylesheets/syntax.css' rel='stylesheet' type='text/css' />
<link href='/stylesheets/responsive.css' rel='stylesheet' type='text/css' />
<link href="/stylesheets/nprogress.css" rel='stylesheet' type='text/css' />
<link href="/stylesheets/duoshuo.css" rel='stylesheet' type='text/css' />

<link href="/feed.xml" rel="alternate" type="application/rss+xml">

<script src='/javascripts/jquery.js' type='text/javascript'></script>
<script src='/javascripts/jquery.particleground.min.js' type='text/javascript'></script>
<script src='/javascripts/nprogress.js' type='text/javascript'></script>
<script src='/javascripts/theater.js' type='text/javascript'></script>

<script src='/javascripts/app.js' type='text/javascript'></script>
<script>
    hljs.initHighlightingOnLoad();
    var duoshuoQuery = {short_name:"kbasha"};

</script>

    </head>
    <body>

        <header>

<div id="blog-desc">
    If you want do, then do.
</div>

</header>

<div id="main">
    <div id="left">
    <a id="head-pic" class="go-back-home" href="/"><img src="../images/kbasha.jpg" alt="Home" width="100" height="100"></a>
    <p>克巴沙</p>
        <ul id="titles" class="titles">
            
        </ul>
    </div>

    <div id="right">
    <a id="scaner-pic" class="go-back-home" href="/"><img src="../images/kbasha_id.jpg" alt="Home" width="172" height="172"></a>
    <p>关注公众号</p>
        
    </div>

        <div id='container'>
            
            <div class="block nav">
    <ul>
    
        <li>
            <span class="line line-top"></span>
            <span class="line line-right"></span>
            <span class="line line-bottom"></span>
            <span class="line line-left"></span>
            <a target="_top" href="/about.html">About</a>
        </li>
    
        <li>
            <span class="line line-top"></span>
            <span class="line line-right"></span>
            <span class="line line-bottom"></span>
            <span class="line line-left"></span>
            <a target="_top" href="/">Blog</a>
        </li>
        <li>
            <span class="line line-top"></span>
            <span class="line line-right"></span>
            <span class="line line-bottom"></span>
            <span class="line line-left"></span>
            <a target="_blank" href="https://github.com/kebasha">GitHub</a>
        </li>
        
    </ul>

</div>


            <section class="paging">
  
    <div class="left">
      <a href="index3.html">
        ‹
      </a>
    </div>
  
    <div class="right">
      <a href="index5.html">
        ›
      </a>
    </div>
  
</section>


            <div class="content">
                <section class='post'>
                    <h1>
                        Hadoop2.x集群编译安装
                        <div class='date'>2016-03-19 17:12:07</div>
                    </h1>
<hr>
<p><strong><code>Hadoop搭建环境</code></strong></p>
<ul>
    <li>主机系统：win 10 64位，CPU：i7；内存：8G</li>
    <li>虚拟软件：VMware</li>
    <li>虚拟机系统：全系标配CentOS6.5 64位；单核；1G内存;(全都使用局域网内独立IP：106、107、108)</li>
    <li>JDK：1.7.0_55 64</li>
    <li>Hadoop: 2.7.2</li>
</ul>

<hr>
<p><strong><code>设置机器名</code></strong></p>
<p>修改/etc/sysconfig/network(重启后生效)</p>
<pre><code class="bash">
[root@hadoop1 /]# vi /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=hadoop1
</code></pre>
<hr>
<p><strong><code>设置hosts文件</code></strong></p>
<p>修改/etc/hosts映射文件(注意：需要重启network; service network restart)</p>
<pre><code class="bash">
[root@hadoop1 /]# vi /etc/hosts
192.168.3.106 hadoop1
192.168.3.107 hadoop2
192.168.3.108 hadoop3
</code></pre>
<hr>
<p><strong><code>关闭防火墙</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# service iptables stop
[root@hadoop1 /]# chkconfig iptables off
</code></pre>
<hr>
<p><strong><code>关闭SElinux</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# vi /etc/selinux/config
SELINUX=disable
</code></pre>
<hr>

<p><strong><code>安装JDK</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# mkdir -p /usr/lib/java
[root@hadoop1 /]# chmod -R 777 /usr/lib/java
[root@hadoop1 /]# tar -zxvf jdk-7u55-linux-x64.tar.gz
[root@hadoop1 /]# vi /etc/profile
export JAVA_HOME=/usr/lib/java/jdk1.7.0_55
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
</code></pre>
<hr>

<p><strong><code>更新OpenSSL(自带的OpenSSL可能存在bug)</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# yum update openssl
</code></pre>
<hr>

<p><strong><code>SSH免密码配置</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# vi /etc/ssh/sshd_config
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile .ssh/authorized_keys
[root@hadoop1 /]# service sshd restart
</code></pre>
<hr>

<p><strong><code>在另外两台机器上做以上同样的操作</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# ......
</code></pre>
<hr>

<p><strong><code>生成私钥和公钥(注意切换用户)</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# su - hadoop
[hadoop@hadoop1 ~]$ cd .ssh
[hadoop@hadoop1 .ssh]$ ssh-keygen -t rsa
#然后一路回车，直到完成
[hadoop@hadoop1 .ssh]$ ls
id_rsa  id_rsa.pub
[hadoop@hadoop1 .ssh]$ cd /home/hadoop/.ssh
#把三个结点的公钥重命名
[hadoop@hadoop1 .ssh]$ cp id_rsa.pub key_hadoop1 
[hadoop@hadoop2 .ssh]$ cp id_rsa.pub key_hadoop2
[hadoop@hadoop3 .ssh]$ cp id_rsa.pub key_hadoop3
</code></pre>
<hr>

<p><strong><code>传输公钥(注意用户以及机器)</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop2 .ssh]$ scp key_hadoop2 hadoop@hadoop1:/home/hadoop/.ssh
[hadoop@hadoop3 .ssh]$ scp key_hadoop3 hadoop@hadoop1:/home/hadoop/.ssh
[hadoop@hadoop1 .ssh]$ ls
id_rsa  id_rsa.pub key_hadoop1 key_hadoop2 key_hadoop3
[hadoop@hadoop1 .ssh]$ cat key_hadoop1 >> hadoop_keys
[hadoop@hadoop1 .ssh]$ cat key_hadoop2 >> hadoop_keys
[hadoop@hadoop1 .ssh]$ cat key_hadoop3 >> hadoop_keys
[hadoop@hadoop1 .ssh]$ scp hadoop_keys hadoop@hadoop2:/home/hadoop/.ssh
[hadoop@hadoop1 .ssh]$ scp hadoop_keys hadoop@hadoop3:/home/hadoop/.ssh

[hadoop@hadoop1 .ssh]$ chmod 400 hadoop_keys
[hadoop@hadoop2 .ssh]$ chmod 400 hadoop_keys
[hadoop@hadoop3 .ssh]$ chmod 400 hadoop_keys

[hadoop@hadoop1 .ssh]$ ssh hadoop1
Last login: Fri Mar 18 08:58:49 2016 from hadoop1
[hadoop@hadoop1 .ssh]$ exit
logout
Connection to hadoop1 closed.
</code></pre>
<hr>

<p><strong><code>maven下载安装</code></strong></p>
<pre><code class="bash">
[root@hadoop1 /]# mkdir -p /app/lib
[root@hadoop1 /]# ce /app/lib
[root@hadoop1 lib]# wget http://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz

[root@hadoop1 lib]# tar -zxvf apache-maven-3.3.9-bin.tar.gz
[root@hadoop1 lib]# vi /etc/profile
export MAVEN_HOME=/app/lib/apache-maven-3.3.9
export PATH=$PATH:$MAVEN_HOME/bin
[root@hadoop1 lib]# source /etc/profile
[root@hadoop1 lib]# mvn -version
</code></pre>
<hr>

<p><strong><code>svn安装</code></strong></p>
<pre><code class="bash">
[root@hadoop1 lib]# yum install svn
</code></pre>
<hr>

<p><strong><code>autoconf automake libtool cmake安装</code></strong></p>
<pre><code class="bash">
[root@hadoop1 lib]# yum install autoconf automake libtool cmake
</code></pre>
<hr>

<p><strong><code>ncurses-devel安装</code></strong></p>
<pre><code class="bash">
[root@hadoop1 lib]# yum install ncurses-devel
</code></pre>
<hr>

<p><strong><code>openssl-devel安装</code></strong></p>
<pre><code class="bash">
[root@hadoop1 lib]# yum install openssl-devel
</code></pre>
<hr>

<p><strong><code>gcc*安装</code></strong></p>
<pre><code class="bash">
[root@hadoop1 lib]# yum install gcc*
</code></pre>
<hr>

<p><strong><code>protobuf安装</code></strong></p>
<p>protobuf官网不好下载，提供个下载地址：http://pan.baidu.com/s/1pJlZubT</p>
<pre><code class="bash">
[root@hadoop1 lib]# tar -xvf protobuf-2.5.0.tar
[root@hadoop1 lib]# cd protobuf-2.5.0
[root@hadoop1 protobuf-2.5.0]# ./configure
[root@hadoop1 protobuf-2.5.0]# make
[root@hadoop1 protobuf-2.5.0]# make check
[root@hadoop1 protobuf-2.5.0]# make install
[root@hadoop1 protobuf-2.5.0]# protoc
Missing input file.
</code></pre>
<hr>

<p><strong><code>编译Hadoop</code></strong></p>
<pre><code class="bash">
[root@hadoop1 lib]# wget http://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2-src.tar.gz
[root@hadoop1 lib]# tar -zxvf hadoop-2.7.2-src.tar.gz
[root@hadoop1 lib]# cd hadoop-2.7.2-src
[root@hadoop1 hadoop-2.7.2-src]# mvn package -Pdist,native -DskipTests -Dtar

</code></pre>
<hr>

<p><strong><code>Hadoop基本配置</code></strong></p>
<pre><code class="bash">
[root@hadoop1 hadoop-2.7.2-src]# cd hadoop-dist/target
[root@hadoop1 target]# mv hadoop-2.7.2 /usr/local/
[root@hadoop1 target]# cd /usr/local/
[root@hadoop1 local]# chown -R hadoop:hadoop hadoop-2.7.2/
[root@hadoop1 /]# su - hadoop
[hadoop@hadoop1 ~]# cd /usr/local/hadoop-2.7.2
[hadoop@hadoop1 hadoop-2.7.2]# mkdir tmp
[hadoop@hadoop1 hadoop-2.7.2]# mkdir name
[hadoop@hadoop1 hadoop-2.7.2]# mkdir data
</code></pre>
<hr>

<p><strong><code>配置hadoop-env.sh</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop-2.7.2]# cd etc/hadoop
[hadoop@hadoop1 hadoop]# vi hadoop-env.sh
export HADOOP_CONF_DIR=/opt/hadoop-2.7.1/etc/hadoop/
export JAVA_HOME=/usr/lib/java/jdk1.7.0_55
export PATH=$PATH:/usr/local/hadoop-2.7.2/bin
[hadoop@hadoop1 hadoop]# source hadoop-env.sh
</code></pre>
<hr>

<p><strong><code>配置yarn-env.sh</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop]# vi yarn-env.sh
export JAVA_HOME=/usr/lib/java/jdk1.7.0_55
[hadoop@hadoop1 hadoop]# source yarn-env.sh
</code></pre>
<hr>

<p><strong><code>配置core-site.xml</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop]# vi core-site.xml
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

&lt;configuration&gt;
    &lt;property&gt;
         &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;fs.defaultFS&lt;/name&gt;
         &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;io.file.buffer.size&lt;/name&gt;
         &lt;value&gt;131072&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
         &lt;value&gt;file:/usr/local/hadoop-2.7.2/tmp&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt;
         &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;
         &lt;value&gt;*&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<hr>

<p><strong><code>hdfs-site.xml</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop]# vi hdfs-site.xml
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

&lt;configuration&gt;
  &lt;property&gt;
      &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
      &lt;value&gt;hadoop1:9001&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
      &lt;value&gt;file:/usr/local/hadoop-2.7.2/name&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
      &lt;value&gt;file:/usr/local/hadoop-2.7.2/data&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
      &lt;name&gt;dfs.replication&lt;/name&gt;
      &lt;value&gt;2&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
      &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
      &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<hr>

<p><strong><code>mapred-site.xml</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml
[hadoop@hadoop1 hadoop]# vi mapred-site.xml
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

&lt;configuration&gt;
    &lt;property&gt;
         &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
         &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
         &lt;value&gt;hadoop1:10020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
         &lt;value&gt;hadoop1:19888&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<hr>

<p><strong><code>配置yarn-site.xml文件</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop]# vi yarn-site.xml
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

&lt;configuration&gt;
    &lt;property&gt;
         &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
         &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
         &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
         &lt;value&gt;hadoop1:8032&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
         &lt;value&gt;hadoop1:8030&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
         &lt;value&gt;hadoop1:8031&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
         &lt;value&gt;hadoop1:8033&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
         &lt;value&gt;hadoop1:8088&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<hr>

<p><strong><code>配置slaves文件</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop]# vi slaves
hadoop2
hadoop3
</code></pre>
<hr>

<p><strong><code>复制hadoop到其他节点</code></strong></p>
<pre><code class="bash">
[root@hadoop2 ~]# mkdir -p /usr/loca/hadoop-2.7.2
[root@hadoop2 ~]# chown -R hadoop /usr/local/hadoop-2.7.2

[root@hadoop3 ~]# mkdir -p /usr/loca/hadoop-2.7.2
[root@hadoop3 ~]# chown -R hadoop /usr/local/hadoop-2.7.2

[hadoop@hadoop1 ~]# cd /usr/local/hadoop-2.7.2
[hadoop@hadoop1 hadoop-2.7.2]# scp -r * hadoop@hadoop2:/usr/local/hadoop-2.7.2
[hadoop@hadoop1 hadoop-2.7.2]# scp -r * hadoop@hadoop3:/usr/local/hadoop-2.7.2
</code></pre>
<hr>

<p><strong><code>格式化namenode</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop-2.7.2]# ./bin/hdfs namenode -format
</code></pre>
<hr>

<p><strong><code>启动hadoop</code></strong></p>
<pre><code class="bash">
[hadoop@hadoop1 hadoop-2.7.2]# ./sbin/start-dfs.sh
Starting namenodes on [hadoop1]
hadoop1: starting namenode, logging to /usr/local/hadoop-2.7.2/logs/hadoop-hadoop-namenode-hadoop1.out
hadoop3: starting datanode, logging to /usr/local/hadoop-2.7.2/logs/hadoop-hadoop-datanode-hadoop3.out
hadoop2: starting datanode, logging to /usr/local/hadoop-2.7.2/logs/hadoop-hadoop-datanode-hadoop2.out
Starting secondary namenodes [hadoop1]
hadoop1: starting secondarynamenode, logging to /usr/local/hadoop-2.7.2/logs/hadoop-hadoop-secondarynamenode-hadoop1.out

[hadoop@hadoop1 hadoop-2.7.2]# jps
16088 Jps
15979 SecondaryNameNode
15791 NameNode

[hadoop@hadoop2 ~]$ jps
2593 DataNode
2684 Jps

[hadoop@hadoop3 ~]$ jps
2666 Jps
2575 DataNode

[hadoop@hadoop1 hadoop-2.7.2]# ./sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop-2.7.2/logs/yarn-hadoop-resourcemanager-hadoop1.out
hadoop3: starting nodemanager, logging to /usr/local/hadoop-2.7.2/logs/yarn-hadoop-nodemanager-hadoop3.out
hadoop2: starting nodemanager, logging to /usr/local/hadoop-2.7.2/logs/yarn-hadoop-nodemanager-hadoop2.out

[hadoop@hadoop1 hadoop-2.7.2]$ jps
16168 ResourceManager
16425 Jps
15979 SecondaryNameNode
15791 NameNode

[hadoop@hadoop2 hadoop-2.7.2]$ jps
2593 DataNode
2722 NodeManager
2821 Jps

</code></pre>
<hr>


                </section>
            </div>

            

        </div>
</div>
        <script>

            jQuery(document).ready(function($) {

                App.mapKeySupport();

                App.initNProgress();

                App.initDuoshuo();

                App.initTheater();
            });

        </script>

    </body>

    <div id="particles">
    <canvas class="pg-canvas"></canvas>
</div>
<script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?93757bc26a1b57b9c6a7771db256d254";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();

    $(function(){

        var $body = $("body");

        $("#particles").css({
                "width":$body.width(), 
                "height":$body.height()
            }).particleground({
                lineColor: '#e0e0e0',
                dotColor: '#EFEFEF',
                lineColor: '#EFEFEF',   
                minSpeedX :0.5,
                maxSpeedX :1,
                minSpeedY : 0.5,
                maxSpeedY :1
            });
    });
</script>
<footer>
  <span class="muted">© kbasha. All Rights Reserved.</span>
  <br>
  <br>
  <!--img src="/images/scribble2.png" alt="scribble" /-->
</footer>


</html>
